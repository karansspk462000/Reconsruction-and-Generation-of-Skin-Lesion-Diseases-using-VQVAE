{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import torchvision\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Step 1: Prepare your dataset\n",
    "class SketchDataset(Dataset):\n",
    "    def __init__(self, csv_file, image_root_dir, sketch_root_dir, transform=None):\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.image_root_dir = image_root_dir\n",
    "        self.sketch_root_dir = sketch_root_dir\n",
    "        self.transform = transform\n",
    "        self.num_sketches = 3594  # Update with the actual number of sketches\n",
    "        self.num_samples = len(self.data_frame)\n",
    "\n",
    "        # Compute variance of the data\n",
    "        self.variance = self.compute_variance().to(device)\n",
    "\n",
    "    def compute_variance(self):\n",
    "        # Load all images and compute variance\n",
    "        data = []\n",
    "        for idx in tqdm(range(len(self))):\n",
    "            img_name = os.path.join(self.image_root_dir, self.data_frame.iloc[idx, 0] + '.jpg')\n",
    "            image = Image.open(img_name).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            data.append(image.numpy())\n",
    "\n",
    "        data = np.array(data)\n",
    "        data = torch.from_numpy(data).to(device)\n",
    "        return torch.var(data / 255.0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.image_root_dir, self.data_frame.iloc[idx, 0] + '.jpg')\n",
    "        sketch_idx = idx % self.num_sketches  # Cyclic indexing for sketches\n",
    "        sketch_name = os.path.join(self.sketch_root_dir, f\"sketch_{sketch_idx + 1}.png\")\n",
    "        \n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        sketch = Image.open(sketch_name)\n",
    "\n",
    "        label = torch.tensor(self.data_frame.iloc[idx, 1:], dtype=torch.float32)\n",
    "\n",
    "        rand_idx = random.randint(0, self.num_samples - 1)\n",
    "        rand_label = torch.tensor(self.data_frame.iloc[rand_idx, 1:], dtype=torch.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            sketch = self.transform(sketch)\n",
    "        \n",
    "        return label, sketch, image, img_name, rand_label, self.variance\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28,28)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.RandomResizedCrop(256, scale=(0.8, 1.0), ratio=(0.75, 1.333)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "train_dataset = SketchDataset(csv_file='/home/cvlab/Karan/A_3/Dataset_A4/Train_labels.csv', \n",
    "                              image_root_dir='/home/cvlab/Karan/A_3/Dataset_A4/Train_data',\n",
    "                              sketch_root_dir='/home/cvlab/Karan/A_3/Dataset_A4/Unpaired_sketch',\n",
    "                              transform=transform)\n",
    "dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers=[16, 32]):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        for i, filters in enumerate(num_layers):\n",
    "            if i == 0:\n",
    "                in_channels = 1  # Assuming grayscale input\n",
    "            else:\n",
    "                in_channels = num_layers[i-1]\n",
    "            self.conv_layers.append(\n",
    "                nn.Conv2d(in_channels, filters, kernel_size=3, padding=1, stride=2)\n",
    "            )\n",
    "        self.z_e = nn.Conv2d(num_layers[-1], d, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for conv_layer in self.conv_layers:\n",
    "            x = nn.functional.relu(conv_layer(x))\n",
    "        z_e = self.z_e(x)\n",
    "        return z_e\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers=[32, 16]):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.convT_layers = nn.ModuleList()\n",
    "        for i, filters in enumerate(num_layers):\n",
    "            if i == 0:\n",
    "                in_channels = d  # Assuming d is defined\n",
    "            else:\n",
    "                in_channels = num_layers[i-1]\n",
    "            self.convT_layers.append(\n",
    "                nn.ConvTranspose2d(in_channels, filters, kernel_size=4, padding=1, stride=2)\n",
    "            )\n",
    "        self.output = nn.ConvTranspose2d(num_layers[-1], 1, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, y):\n",
    "        for convT_layer in self.convT_layers:\n",
    "            y = nn.functional.relu(convT_layer(y))\n",
    "        decoded = torch.sigmoid(self.output(y))\n",
    "        return decoded\n",
    "\n",
    "# Usage\n",
    "d = 64  # Define the value of d\n",
    "encoder = Encoder()\n",
    "decoder = Decoder()\n",
    "inputs = torch.randn(1, 1, 28, 28)  # Assuming input shape (batch_size, channels, height, width)\n",
    "z_e = encoder(inputs)\n",
    "decoded = decoder(z_e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, k):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        self.k = k\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # inputs shape: (batch_size, w, h, d)\n",
    "        batch_size, w, h, d = inputs.size()\n",
    "        # Flatten inputs to shape (batch_size * w * h, d)\n",
    "        inputs_flat = inputs.view(-1, d)\n",
    "        \n",
    "        # Calculate distances between inputs and codebook\n",
    "        distances = torch.norm(inputs_flat.unsqueeze(1) - self.codebook.unsqueeze(0), dim=-1)\n",
    "        # Find indices of nearest codebook entries\n",
    "        indices = torch.argmin(distances, dim=-1)\n",
    "        \n",
    "        # Reshape indices to match inputs shape\n",
    "        indices = indices.view(batch_size, w, h)\n",
    "        \n",
    "        # Lookup nearest codebook entries\n",
    "        quantized = torch.gather(self.codebook, 0, indices.unsqueeze(-1).expand(-1, -1, -1, d))\n",
    "        \n",
    "        return quantized, indices\n",
    "\n",
    "    def init_codebook(self, d):\n",
    "        # Initialize codebook with random values\n",
    "        self.codebook = nn.Parameter(torch.randn(self.k, d))\n",
    "\n",
    "# Usage\n",
    "k = 10  # Set the number of codebook entries\n",
    "vq = VectorQuantizer(k)\n",
    "inputs = torch.randn(1, 28, 28, 64)  # Assuming input shape (batch_size, width, height, channels)\n",
    "vq.init_codebook(inputs.size(-1))\n",
    "quantized, indices = vq(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers=[16, 32]):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        for i, filters in enumerate(num_layers):\n",
    "            if i == 0:\n",
    "                in_channels = 1  # Assuming grayscale input\n",
    "            else:\n",
    "                in_channels = num_layers[i-1]\n",
    "            self.conv_layers.append(\n",
    "                nn.Conv2d(in_channels, filters, kernel_size=3, padding=1, stride=2)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for conv_layer in self.conv_layers:\n",
    "            x = nn.functional.relu(conv_layer(x))\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers=[32, 16]):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.convT_layers = nn.ModuleList()\n",
    "        for i, filters in enumerate(num_layers):\n",
    "            if i == 0:\n",
    "                in_channels = d  # Assuming d is defined\n",
    "            else:\n",
    "                in_channels = num_layers[i-1]\n",
    "            self.convT_layers.append(\n",
    "                nn.ConvTranspose2d(in_channels, filters, kernel_size=4, padding=1, stride=2)\n",
    "            )\n",
    "        self.output = nn.ConvTranspose2d(num_layers[-1], 1, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, y):\n",
    "        for convT_layer in self.convT_layers:\n",
    "            y = nn.functional.relu(convT_layer(y))\n",
    "        decoded = torch.sigmoid(self.output(y))\n",
    "        return decoded\n",
    "\n",
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, k, d):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        self.k = k\n",
    "        self.d = d\n",
    "        self.codebook = nn.Parameter(torch.randn(k, d))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch_size, w, h, d = inputs.size()\n",
    "        inputs_flat = inputs.view(-1, d)\n",
    "        distances = torch.norm(inputs_flat.unsqueeze(1) - self.codebook.unsqueeze(0), dim=-1)\n",
    "        indices = torch.argmin(distances, dim=-1)\n",
    "        indices = indices.view(batch_size, w, h)\n",
    "        quantized = torch.gather(self.codebook, 0, indices.unsqueeze(-1).expand(-1, -1, -1, d))\n",
    "        return quantized, indices\n",
    "\n",
    "    def sample(self, indices):\n",
    "        quantized = torch.gather(self.codebook, 0, indices.unsqueeze(-1).expand(-1, -1, -1, self.d))\n",
    "        return quantized\n",
    "\n",
    "# Building VQ-VAE\n",
    "def build_vqvae(k, d, input_shape=(1, 28, 28), num_layers=[16, 32]):\n",
    "    global SIZE\n",
    "\n",
    "    # Encoder\n",
    "    encoder = Encoder(num_layers=num_layers)\n",
    "\n",
    "    # Vector Quantization\n",
    "    vector_quantizer = VectorQuantizer(k, d)\n",
    "\n",
    "    # Decoder\n",
    "    decoder = Decoder(num_layers=num_layers[::-1])\n",
    "\n",
    "    SIZE = input_shape[1] // (2 ** (len(num_layers)))\n",
    "\n",
    "    # VQ-VAE Model (training)\n",
    "    class SamplingLayer(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(SamplingLayer, self).__init__()\n",
    "\n",
    "        def forward(self, indices):\n",
    "            z_q = vector_quantizer.sample(indices)\n",
    "            return z_q\n",
    "\n",
    "    sampling_layer = SamplingLayer()\n",
    "\n",
    "    class StraightThroughEstimator(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(StraightThroughEstimator, self).__init__()\n",
    "\n",
    "        def forward(self, z_q, z_e):\n",
    "            return z_q + (z_e - z_q).detach()\n",
    "\n",
    "    straight_through = StraightThroughEstimator()\n",
    "\n",
    "    vq_vae = nn.Sequential(\n",
    "        encoder,\n",
    "        vector_quantizer,\n",
    "        sampling_layer,\n",
    "        straight_through,\n",
    "        decoder\n",
    "    )\n",
    "\n",
    "    # VQ-VAE Model (inference)\n",
    "    class VQVAESampler(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(VQVAESampler, self).__init__()\n",
    "\n",
    "        def forward(self, indices):\n",
    "            z_q = vector_quantizer.sample(indices)\n",
    "            generated = decoder(z_q)\n",
    "            return generated\n",
    "\n",
    "    vq_vae_sampler = VQVAESampler()\n",
    "\n",
    "    # Getter to easily access the codebook for visualization\n",
    "    def get_vq_vae_codebook():\n",
    "        return vector_quantizer.codebook.detach().cpu().numpy()\n",
    "\n",
    "    return vq_vae, vq_vae_sampler, encoder, decoder, get_vq_vae_codebook\n",
    "\n",
    "# Hyperparameters\n",
    "NUM_LATENT_K = 10                 # Number of codebook entries\n",
    "NUM_LATENT_D = 64                 # Dimension of each codebook entries\n",
    "BETA = 1.0                        # Weight for the commitment loss\n",
    "\n",
    "# INPUT_SHAPE = x_train.shape[1:]\n",
    "SIZE = None                       # Spatial size of latent embedding\n",
    "                                  # will be set dynamically in `build_vqvae\n",
    "\n",
    "VQVAE_BATCH_SIZE = 128            # Batch size for training the VQVAE\n",
    "VQVAE_NUM_EPOCHS = 20             # Number of epochs\n",
    "VQVAE_LEARNING_RATE = 3e-4        # Learning rate\n",
    "VQVAE_LAYERS = [16, 32]           # Number of filters for each layer in the encoder\n",
    "\n",
    "PIXELCNN_BATCH_SIZE = 128         # Batch size for training the PixelCNN prior\n",
    "PIXELCNN_NUM_EPOCHS = 10          # Number of epochs\n",
    "PIXELCNN_LEARNING_RATE = 3e-4     # Learning rate\n",
    "PIXELCNN_NUM_BLOCKS = 12          # Number of Gated PixelCNN blocks in the architecture\n",
    "PIXELCNN_NUM_FEATURE_MAPS = 32    # Width of each PixelCNN block\n",
    "vq_vae, vq_vae_sampler, encoder, decoder, get_vq_vae_codebook = build_vqvae(k, d, input_shape, num_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def mse_loss(ground_truth, predictions):\n",
    "    return torch.mean((ground_truth - predictions)**2)\n",
    "\n",
    "def latent_loss(dummy_ground_truth, outputs):\n",
    "    global BETA\n",
    "    del dummy_ground_truth\n",
    "    z_e, z_q = torch.split(outputs, outputs.size(-1) // 2, dim=-1)\n",
    "    vq_loss = torch.mean((z_e.detach() - z_q)**2)\n",
    "    commit_loss = torch.mean((z_e - z_q.detach())**2)\n",
    "    latent_loss = vq_loss + BETA * commit_loss\n",
    "    return latent_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def zq_norm(y_true, y_pred):\n",
    "    _, z_q = torch.split(y_pred, y_pred.size(-1) // 2, dim=-1)\n",
    "    return torch.mean(torch.norm(z_q, dim=-1))\n",
    "\n",
    "def ze_norm(y_true, y_pred):\n",
    "    z_e, _ = torch.split(y_pred, y_pred.size(-1) // 2, dim=-1)\n",
    "    return torch.mean(torch.norm(z_e, dim=-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Assuming x_train is your training data, convert it to a PyTorch tensor\n",
    "# x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader for batch training\n",
    "train_loader = DataLoader(dataloader, batch_size=VQVAE_BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(vq_vae.parameters(), lr=VQVAE_LEARNING_RATE)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(VQVAE_NUM_EPOCHS):\n",
    "    vq_vae.train()\n",
    "    total_loss = 0.0\n",
    "    total_mse_loss = 0.0\n",
    "    total_latent_loss = 0.0\n",
    "    total_zq_norm = 0.0\n",
    "    total_ze_norm = 0.0\n",
    "    \n",
    "    for batch_idx, (inputs,) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        reconstructed, codes = vq_vae(inputs)\n",
    "        \n",
    "        # Compute MSE loss\n",
    "        mse_loss_val = mse_loss(inputs, reconstructed)\n",
    "        \n",
    "        # Compute latent loss\n",
    "        latent_loss_val = latent_loss(None, codes)\n",
    "        \n",
    "        # Compute total loss\n",
    "        loss = mse_loss_val + latent_loss_val\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Compute metrics\n",
    "        with torch.no_grad():\n",
    "            zq_norm_val = zq_norm(None, codes)\n",
    "            ze_norm_val = ze_norm(None, codes)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_mse_loss += mse_loss_val.item()\n",
    "        total_latent_loss += latent_loss_val.item()\n",
    "        total_zq_norm += zq_norm_val.item()\n",
    "        total_ze_norm += ze_norm_val.item()\n",
    "    \n",
    "    # Compute average losses and metrics\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    avg_mse_loss = total_mse_loss / len(train_loader)\n",
    "    avg_latent_loss = total_latent_loss / len(train_loader)\n",
    "    avg_zq_norm = total_zq_norm / len(train_loader)\n",
    "    avg_ze_norm = total_ze_norm / len(train_loader)\n",
    "    \n",
    "    # Print epoch results\n",
    "    print(f\"Epoch {epoch + 1}/{VQVAE_NUM_EPOCHS}:\")\n",
    "    print(f\"  Loss: {avg_loss:.4f}, MSE Loss: {avg_mse_loss:.4f}, Latent Loss: {avg_latent_loss:.4f}\")\n",
    "    print(f\"  z_q Norm: {avg_zq_norm:.4f}, z_e Norm: {avg_ze_norm:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
